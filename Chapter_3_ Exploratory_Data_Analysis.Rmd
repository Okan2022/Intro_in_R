---
title: "Exploratory Data Analysis"
author: "Okan Sarioglu <br> Leon Siefken"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    css: css/lab.css
  pdf_document:
    toc: yes
  html_notebook:
    toc: true
    toc_float: true
    css: css/lab.css
---

# Chapter 3: Exploratory Data Analysis

## 1. New Packages: The Psych-Package 

Initially developed for psychological research, the `psych` Package will prove helpful for us in political science. Next to data visualization, factor analysis, reliability analysis and correlation analysis, this package will also help us to get an overview of hour datasets that we're working with. Getting a general look at your data is essential for getting a deeper understanding of what you are working with.
We will also need the `tidyverse` and `haven`.

```{r}
install.packages("psych")
install.packages("ineq")
install.packages("corrplot")
```



```{r}
pacman::p_load("tidyverse","haven","psych","ineq","corrplot")
```

## 2. Essential Datasets on Political Parties

Analyzing political parties, their positions and their election results is among the most essential tasks of political science. And to our luck, an abundance of this data has been and is collected by other scientists. To name a few, ParlGov, Manifesto Project, CHES, and POPPA are among the most commonly used datasets on said aspects of parties in (mainly) Europe. To get an understanding of exploratory data analysis, we will make use of the POPPA and the ParlGov datasets. 

- The POPPA data comprises an Expert Survey on European parties' positions on a number of different ideological dimensions, with a focus on measuring a party's populism. [Website](http://poppa-data.eu/) 

- ParlGov has collected data on a variety of countries, their elections, their outcome and the resulting cabinets. Generally, they provide an expansive data collection on elections and their results. [Website](https://www.parlgov.org/)

```{r data}
pop <- read_dta("data/party_means.dta")
parl <- read.csv("data/parlgov_election.csv")
```

### 2.1 Merging the Datasets

To get a complete dataset on party positions and the parties' vote shares, we will need to merge the two datasets. Do you remember how to do that? To our luck, it just so happens that the POPPA data includes a variable that has the ParlGov ID of every party, making merging easy for us. However, as POPPA only measures party positions at around 2018, we have to limit ParlGov to this timeframe. 

```{r merge}
parl18 <- parl %>% 
  filter(election_date <= 2019 & election_date >= 2017) 

comp <- left_join(parl18, pop, by = c("party_id" = "parlgov_id"))
```

## 3. A First Look at the Data

Now that our dataset is complete, we have a dataset that includes the vote shares of parties as well as their political positions according to expert surveys. Time to get a good look at what data exactly we are dealing with! Let's make use of the `psych` package. 

```{r describe}
describe(comp)
```

The `describe()` function gives a great overview over each variable in the dataset. `Vars` simply shows us a count of the variables. Going to the last page, we can see that we have 42 variables in our dataset. `n` shows us the number of observations for each variable. Now, more interesting to us is the column named `mean`. This column shows us the arithmetic mean of each variable, something that of course only works with numeric variables. It shows us, that the average vote share lies at around 10.24%. However, this alone tells us relatively little. We also need the standard deviation (remember, the average deviation from the mean) as well. This is told by `sd`. We can see that a parties vote share lies at a mean of 10.24% with a standard deviation of +/- 10.98%, a rather big number in comparison to the mean. We can, from this, conclude that a mean will not get us very far, as the vote share seems to vary a lot. Three other important, that can show us how much the vote share actually varies, are `min`, `max`, and `range`, which tell us the minimum measured vote share, the highest measured vote share and the range between the minimum and maximum. Thus, we see that the lowest measured vote share is 0.21%, the highest is 55.04% and the range between the two is at 54.83% (max-min = range). We can of course also look at these values seperately. The values are the same

```{r measure}
mean(comp$vote_share, na.rm = T) #For these functions to work, we will need to exclude missing values
sd(comp$vote_share, na.rm = T)
median(comp$vote_share, na.rm = T)
min(comp$vote_share, na.rm = T)
max(comp$vote_share, na.rm = T)
max(comp$vote_share, na.rm = T) - min(comp$vote_share, na.rm = T)
```

### 3.1 A Look at Distributions

Now we got a first look at our data and specifically the vote share of the included parties. Doing this, we could see that the parties' vote shares vary a lot and that summarizing statistics can not tell us alot. We need to look at the distribution of the variable to get a better understanding of vote shares. Now, we will get into visualizations in the next sessions, however, it is very helpful to look at a variables distribution using a simple histogram. 

```{r distribution}
hist(comp$vote_share, breaks = 30) #Breaks lets us adjust the number of bins in the plot
```

We can now see that, while the vote share does vary a lot, most parties receive a vote share at around 0 to 2%. The highest amount of parties lies at around 0% to 13%. We call this a right-skewed distribution- Knowing the destribution of your variable becomes essential when you get into inferential statistics in QM and AQM. But the fact that most parties lie in this range can already be seen by our `describe()` function, by looking at the median. You can imagine the median like this: lay all measure vote shares next to each other, ordered by highest values, then the median is the value exactly in the middle of this. This way of measuring a variables distribution is known as measuring quantiles. Arranging the variables from lowest to highest, then looking at specific parts of this arrangement. Let us look at a function that can show us the quantiles.

```{r quantiles}
quantile(comp$vote_share, na.rm = T)
```

How to interpret this? As explained earlier, imagine all observations arranged from lowest to highest. Now we make three cuts into this arrangement: at 25%, 50%, and 100%. Now we can see which values lay at the borders. But we can also intrpret it like this: 25% percent of parties receive a share of 2.4% or lower, thus 75% of parties receive a vote share of 2.4% or higher! The same we can do with the other quantiles. 75% of parties had a vote share of 13% or lower, while 25% of parties had a vote share of 13% or higher. These are interesting insights! While it is common to have 5 values as quintiles, you can also set the borders yourself.

```{r qunit}
quantile(comp$vote_share,probs = c(0.2,0.4,0.6,0.8), na.rm = T)
```

How would you interpret this?

And lastly, another common measure is the Inter Quartile Range (IQR), which simply measures the range between the 25% and the 75% quantiles, to get an idea of the distribution of the variable. Note: It will still say 75%, which is, however, not correct of course. Just ignore it. 

```{r IQR}
quantile(comp$vote_share, 0.75, na.rm = T) - quantile(comp$vote_share, 0.25, na.rm = T) 
```

### 3.2 Summary of Specific Variables

Perhaps you just want to look at a specific variable, you can simply use the following function.

```{r summary}
summary(comp$vote_share)
```

## 4. The Gini-Coefficient

Another commonly used measure, specifically when trying to judge if there is inequality in a distribution, is the Gini-Coefficient. Complete equality is imagined by a straight vertical line. The Lorenzcurve then portraits the actual distribution. Now, to measure the inequality of a distribution, the Gini-Coefficient, we estimate the size of the gap between these two lines. The closer the value gets to 1, the higher the unequality, while a value closer to 0 indicates a completely equal distribution. In R, we can easily estimate this as follows.

```{r gini}
Gini(comp$vote_share)
```
However, this becomes more interesting when comparing different countries.

```{r gini comp}
unique(comp$country_name) #Shows us all unique values in this variable!
neth <- comp %>% 
  filter(comp$country_name == "Netherlands")

uk <- comp %>% 
  filter(comp$country_name == "United Kingdom")

ger <- comp %>% 
  filter(comp$country_name == "Germany")

Gini(neth$vote_share)
Gini(uk$vote_share)
Gini(ger$vote_share)
```

As expected, the UK with it's majoritarian electoral system has the highest inequality when it comes to the distribution of vote shares among parties, while the countries with representative system lie at a gini of around 40%, which is lower than the general Gini we calculated before.


## 5. Frequency Tables

Another way to better understand the variables your dealing with and do some first and basic descriptive statistics, are frequency tables. The most common ones you will use and come across are normal frequency tables and proportioanl tables. Lets start with the first.

### 5.1 Standard Tables

The basic frequency table in R will show you the respective values or categories of a variable as well as how often it has been observed in the data. Thus, these tables make more sense for categorical variables. To start, let us look at the vote shares again. However, the tables work best on categorical variables, so let us create a categorical seat share variable first.

```{r freq}
comp$cat_vote <- cut(comp$vote_share,
                       breaks=c(0, 5, 10, 15, 20, 25, 50, 100),
                       labels=c('0-4','5-9', '10-14', '15-19', '20-24', '25-49', '50+'))

table(comp$cat_vote)
```
We can now see that most parties have a vote share between 0% and 4%, and generally lower vote shares, while there are also a large number of parties that receive between 25 to 49 percent, likely in majoritarian systems. 
We can also create a table that cumulates the values from column to column.

```{r cumsum}
t1 <- table(comp$cat_vote)
cumsum(t1)
```
Whith this we can say: 77 parties in the data received between 0% and 4% of votes. 117 parties received 9% or less votes. 142 parties received 14% or less and so on... 

### 5.2 Proportional Tables

To get a better understanding of this hinted at bias, we can calculated proportional tables to find out which percentage of data is made up by each country.

```{r prop}
prop.table(t1)
t2 <- prop.table(t1)
```
The interpretation here is self-explanatory. 42% of parties receive between 0% and 4% of votes. 22% get between 5% and 9%... Again, we can create a cummulative table as well.

```{r cumsum prop}
cumsum(t2)
```

## 6. Crosstabulation

Now, let us get to something more interesting! We now learned a lot about how to get a first look at single variables in our datasets. However, usually in science, we care about how two or more variables are connected and possibly correlate. But before we get into extensive inferential statistics, here too, it can be helpful to get a first exploratory look at the data at hand! One way to do so is crosstabulation. Cross tables let us look at how the distribution of two variables together. Let's look at an example.

```{r crosstab}
xtabs(~comp$vote_share + comp$seats)
```
Hmm... Running this code was not very helpful, was it? One way to get a better table is to turn both variables into categorical variables. As we already transformed the vote share, we only need to transform the seats. But first, we will make use of our data wrangling skills to create a seat share variable, that tells us which percentage of seats in each country each party got. 

```{r cats}
comp <- comp %>% 
  group_by(country_name) %>% 
  mutate(seat_share = seats / sum(seats))

comp$cat_seat <- cut(comp$seat_share,
                       breaks=c(0, 0.1, 0.2, 0.3, 0.4, 
                                0.5, 0.6, 0.7, 0.8, 0.9, 1),
                       labels=c('0-9', '10-19', '20-29',
                                '30-39', '40-49', '50-59',
                                '60-69', '70-79',
                                '80-89', '90-100'))

t3 <- xtabs(~comp$cat_vote+comp$cat_seat)
t3
```

Much better! What can we see here? There appears to be a connection between the percentage of votes and the share of seats a party received. The higher the vote share, the higher the seat share as well! Honestly, not very surprising but hopefully it makes the advantage of cross-tables more understandable. Again, we can transform this into a proportional table.

```{r prop x}
prop.table(t3)
```

Another interesting statistic we can look at now are the margins. We can look at row margins by adding `margin = 1` and column margins by adding `margin = 2` to the command.

```{r margins}
prop.table(t3, margin = 1)
prop.table(t3, margin = 2)
```

Row margins will add up to 100% in each row, while column margins add up to 100% in each column. Looking at row margins we can say: Of those parties who received between 5% and 9% of votes, 13% also received between 10% and 19% of seats in the parliament. For column margins we can say: Of those parties who received between 20% and 29% of seats(!), 38% also received between 20% and 24% of votes in the respective election. Quite useful!

## 7. Correlation Matrcies

We now found that vote share and seat share are correlated. Great. However, we can take this two steps further! We can calculate a correlation coefficient between the two variables with a simple command. The basic command calculates the Pearson R, but we can also tell the command to give us the Spearman Coefficient and Kendall's Tau-b.

```{r corr}
#We can only include observations without NA
cor(comp$seat_share, comp$vote_share, use = "complete.obs") 
cor(comp$seat_share, comp$vote_share, method = "spearman", use = "complete.obs")
cor(comp$seat_share, comp$vote_share, method = "kendall", use = "complete.obs")
```
All three coefficients show us that vote share and seat share are positively correlated. But this, again, is no surprise... How about we check the correlations of all (numeric) variables in our data to see if we do find a surprise? We can do so by calculating a correlation martix. This matrix contains the correlation coefficients between all variables in the data, by simply inputting the dataset into the previous command. However, we have to exclude all variables that are not numeric.

```{r mat}
num <- comp[,c(5,6,7,seq(24,39,1),42,44)] #We select only the numeric variables in the dataset.

#Do you remember where to find the respective number of each variable?

cor(num, use = "complete.obs")
```

Okay, overwhelmingly huge table! However, we can make this more appealing. Take this as a sneak-peek to our next session and let us visualize the correlation matrix. For this, we needed the `corrplot`package.

```{r corr mat col}
cormat <- cor(num, use = "complete.obs")

corrplot(cormat, type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)
#with tl.col we can set the color of the text, with tl.srt we can angle the top text to 45 degrees
```

And here we have a visualization of our correlation matrix. The size and the gradiant of the color shows you how strong the effect is, with red meaning a negative correlation, while blue means a positive correlation, as indicated by the color legend on the right. We can see at lot of correlations between the variables of the POPPA data. For example, the dimensions of populism positively correlate to each other. We can also spot some slight positive relations between the vote and seat share and a more personalized approach to politics of a party, indicating that these parties seem to be more successful in elections. A matrix like this might offer some inspriation to further investigate relationships in the data.

## 8. Concluding Remarks

In this session we have learned how to get a first overview of the data we are dealing with. We know how to compute the common descriptive measures in statistics such as the mean, standard deviation, quantiles and more. We know how to get an impression of how the variables we are dealing with are distributed. We also have seen some statistical measures such as the Gini-Coefficient and correlation coefficients. Finally, we know how to compute correlation matrices and even visualize them in an appealing format. 

In the next session we will get more familiar with the process of visualizing your data with `ggplot2` and make them look suitable for publications!

## Exercises

### Exercise 1: Power to the People

The main contribution of the POPPA data is it's measure of the parties' populist sentiments. Get an overview of this variable (`populism`). What is it's mean, median, and quantiles? How high is the IQR? And lastly, create a histogram with 40 bins and take a look at the variables distribution. 

```{r populi}

```

### Exercise 2: Anti-Immigration Sentiment, That's what "the People" want...?

Judging from our correlation plot, there appears to be a correlation between the opinion on immigration and how much a party claims to represent the "general will" of the citizens. Explore this correlation by computing the different correlation coefficients that we met today. How would you interpret this correlation?

```{r thepeop}

```




